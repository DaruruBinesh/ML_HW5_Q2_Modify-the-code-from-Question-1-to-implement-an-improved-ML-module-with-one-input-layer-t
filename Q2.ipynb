{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "tsABZdz3aEXR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bt1 = pd.read_csv(\"Q2_train_data.csv\")\n",
        "bvs1 = pd.read_csv(\"Q2_dev_data.csv\")\n",
        "bts1 = pd.read_csv(\"Q2_test_data.csv\")\n",
        "bps1 = np.array(pd.read_csv(\"Q2_pred_data.csv\"))"
      ],
      "metadata": {
        "id": "svW-3xmDaMTT"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "\ttf.keras.layers.InputLayer(input_shape=(4), name=\"InputLayer\"),\n",
        "\ttf.keras.layers.Dense(6, activation=\"relu\", name=\"HiddenLayer.1\"),\n",
        "\ttf.keras.layers.Dropout(0.1),\n",
        "\ttf.keras.layers.Dense(4, activation=\"relu\", name=\"HiddenLayer.2\"),\n",
        "\ttf.keras.layers.Dense(2, activation=\"sigmoid\", name=\"OutputLayer\")\n",
        "])\n",
        "model.compile(optimizer=\"sgd\", loss=\"mse\")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncy_mI3naR6U",
        "outputId": "55c59959-ce8e-4e44-831f-01c3875fef61"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " HiddenLayer.1 (Dense)       (None, 6)                 30        \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 6)                 0         \n",
            "                                                                 \n",
            " HiddenLayer.2 (Dense)       (None, 4)                 28        \n",
            "                                                                 \n",
            " OutputLayer (Dense)         (None, 2)                 10        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 68\n",
            "Trainable params: 68\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LossThresholdCallback(tf.keras.callbacks.Callback):\n",
        "\tdef __init__(self, threshold):\n",
        "\t\tsuper(LossThresholdCallback, self).__init__()\n",
        "\t\tself.threshold = threshold\n",
        "\tdef on_epoch_end(self, epoch, logs=None):\n",
        "\t\tif logs['val_loss'] <= self.threshold:\n",
        "\t\t\tself.model.stop_training=True\n",
        "\n",
        "print(\"TRAINING ML PROGRAM\")\n",
        "model.fit(\n",
        "\tx=np.array(bts1.loc[:,\"X3\":\"X0\"]),\n",
        "\ty=np.array(bts1.loc[:,\"Y1\":\"Y0\"]),\n",
        "\tvalidation_data=(\n",
        "\t\tnp.array(bvs1.loc[:,\"X3\":\"X0\"]),\n",
        "\t\tnp.array(bvs1.loc[:,\"Y1\":\"Y0\"])\n",
        "\t),\n",
        "\tbatch_size=5,\n",
        "\t\n",
        "\tepochs=260,\n",
        "\tcallbacks=[LossThresholdCallback(0.03)]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNDpm43sa2vN",
        "outputId": "def8d860-6678-4bac-809d-f6d494c1c735"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING ML PROGRAM\n",
            "Epoch 1/260\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 0.2646 - val_loss: 0.2578\n",
            "Epoch 2/260\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.2645 - val_loss: 0.2578\n",
            "Epoch 3/260\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.2644 - val_loss: 0.2578\n",
            "Epoch 4/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2643 - val_loss: 0.2578\n",
            "Epoch 5/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2641 - val_loss: 0.2578\n",
            "Epoch 6/260\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.2640 - val_loss: 0.2578\n",
            "Epoch 7/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2639 - val_loss: 0.2578\n",
            "Epoch 8/260\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.2638 - val_loss: 0.2578\n",
            "Epoch 9/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2637 - val_loss: 0.2578\n",
            "Epoch 10/260\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2636 - val_loss: 0.2578\n",
            "Epoch 11/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2635 - val_loss: 0.2578\n",
            "Epoch 12/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2634 - val_loss: 0.2578\n",
            "Epoch 13/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2633 - val_loss: 0.2578\n",
            "Epoch 14/260\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.2632 - val_loss: 0.2578\n",
            "Epoch 15/260\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2631 - val_loss: 0.2578\n",
            "Epoch 16/260\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2630 - val_loss: 0.2578\n",
            "Epoch 17/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2629 - val_loss: 0.2578\n",
            "Epoch 18/260\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2628 - val_loss: 0.2578\n",
            "Epoch 19/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 20/260\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2626 - val_loss: 0.2578\n",
            "Epoch 21/260\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2625 - val_loss: 0.2578\n",
            "Epoch 22/260\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2624 - val_loss: 0.2578\n",
            "Epoch 23/260\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2623 - val_loss: 0.2578\n",
            "Epoch 24/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2622 - val_loss: 0.2578\n",
            "Epoch 25/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2621 - val_loss: 0.2578\n",
            "Epoch 26/260\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2620 - val_loss: 0.2578\n",
            "Epoch 27/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2619 - val_loss: 0.2578\n",
            "Epoch 28/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2618 - val_loss: 0.2578\n",
            "Epoch 29/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2617 - val_loss: 0.2578\n",
            "Epoch 30/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2616 - val_loss: 0.2578\n",
            "Epoch 31/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2615 - val_loss: 0.2578\n",
            "Epoch 32/260\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2614 - val_loss: 0.2578\n",
            "Epoch 33/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2613 - val_loss: 0.2578\n",
            "Epoch 34/260\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2612 - val_loss: 0.2578\n",
            "Epoch 35/260\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 36/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2611 - val_loss: 0.2578\n",
            "Epoch 37/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2610 - val_loss: 0.2578\n",
            "Epoch 38/260\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2609 - val_loss: 0.2578\n",
            "Epoch 39/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2608 - val_loss: 0.2578\n",
            "Epoch 40/260\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.2607 - val_loss: 0.2578\n",
            "Epoch 41/260\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2606 - val_loss: 0.2578\n",
            "Epoch 42/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2605 - val_loss: 0.2578\n",
            "Epoch 43/260\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2604 - val_loss: 0.2578\n",
            "Epoch 44/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2603 - val_loss: 0.2578\n",
            "Epoch 45/260\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.2602 - val_loss: 0.2578\n",
            "Epoch 46/260\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.2601 - val_loss: 0.2578\n",
            "Epoch 47/260\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.2600 - val_loss: 0.2578\n",
            "Epoch 48/260\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.2599 - val_loss: 0.2578\n",
            "Epoch 49/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2598 - val_loss: 0.2578\n",
            "Epoch 50/260\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.2597 - val_loss: 0.2578\n",
            "Epoch 51/260\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2596 - val_loss: 0.2578\n",
            "Epoch 52/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2596 - val_loss: 0.2578\n",
            "Epoch 53/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2595 - val_loss: 0.2578\n",
            "Epoch 54/260\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2594 - val_loss: 0.2578\n",
            "Epoch 55/260\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2593 - val_loss: 0.2578\n",
            "Epoch 56/260\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.2592 - val_loss: 0.2578\n",
            "Epoch 57/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2591 - val_loss: 0.2578\n",
            "Epoch 58/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2590 - val_loss: 0.2578\n",
            "Epoch 59/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2589 - val_loss: 0.2578\n",
            "Epoch 60/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2588 - val_loss: 0.2578\n",
            "Epoch 61/260\n",
            "1/1 [==============================] - 0s 163ms/step - loss: 0.2587 - val_loss: 0.2578\n",
            "Epoch 62/260\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.2586 - val_loss: 0.2578\n",
            "Epoch 63/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2585 - val_loss: 0.2578\n",
            "Epoch 64/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 65/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2584 - val_loss: 0.2578\n",
            "Epoch 66/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2583 - val_loss: 0.2578\n",
            "Epoch 67/260\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2582 - val_loss: 0.2578\n",
            "Epoch 68/260\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2581 - val_loss: 0.2578\n",
            "Epoch 69/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2581 - val_loss: 0.2578\n",
            "Epoch 70/260\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2580 - val_loss: 0.2578\n",
            "Epoch 71/260\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2579 - val_loss: 0.2578\n",
            "Epoch 72/260\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2578 - val_loss: 0.2578\n",
            "Epoch 73/260\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2577 - val_loss: 0.2578\n",
            "Epoch 74/260\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.2576 - val_loss: 0.2578\n",
            "Epoch 75/260\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2575 - val_loss: 0.2578\n",
            "Epoch 76/260\n",
            "1/1 [==============================] - 0s 179ms/step - loss: 0.2574 - val_loss: 0.2578\n",
            "Epoch 77/260\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.2573 - val_loss: 0.2578\n",
            "Epoch 78/260\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2572 - val_loss: 0.2578\n",
            "Epoch 79/260\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.2571 - val_loss: 0.2578\n",
            "Epoch 80/260\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2571 - val_loss: 0.2578\n",
            "Epoch 81/260\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.2570 - val_loss: 0.2578\n",
            "Epoch 82/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2569 - val_loss: 0.2578\n",
            "Epoch 83/260\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2568 - val_loss: 0.2578\n",
            "Epoch 84/260\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.2567 - val_loss: 0.2578\n",
            "Epoch 85/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2566 - val_loss: 0.2578\n",
            "Epoch 86/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2565 - val_loss: 0.2578\n",
            "Epoch 87/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2564 - val_loss: 0.2578\n",
            "Epoch 88/260\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.2564 - val_loss: 0.2578\n",
            "Epoch 89/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2563 - val_loss: 0.2578\n",
            "Epoch 90/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 91/260\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.2562 - val_loss: 0.2578\n",
            "Epoch 92/260\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.2561 - val_loss: 0.2578\n",
            "Epoch 93/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 94/260\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2560 - val_loss: 0.2578\n",
            "Epoch 95/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2559 - val_loss: 0.2578\n",
            "Epoch 96/260\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.2558 - val_loss: 0.2578\n",
            "Epoch 97/260\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.2557 - val_loss: 0.2578\n",
            "Epoch 98/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2557 - val_loss: 0.2578\n",
            "Epoch 99/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2556 - val_loss: 0.2578\n",
            "Epoch 100/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2555 - val_loss: 0.2578\n",
            "Epoch 101/260\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.2554 - val_loss: 0.2578\n",
            "Epoch 102/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2553 - val_loss: 0.2578\n",
            "Epoch 103/260\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.2552 - val_loss: 0.2578\n",
            "Epoch 104/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 105/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2551 - val_loss: 0.2578\n",
            "Epoch 106/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2551 - val_loss: 0.2578\n",
            "Epoch 107/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2550 - val_loss: 0.2578\n",
            "Epoch 108/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2549 - val_loss: 0.2578\n",
            "Epoch 109/260\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2548 - val_loss: 0.2578\n",
            "Epoch 110/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2547 - val_loss: 0.2578\n",
            "Epoch 111/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2546 - val_loss: 0.2578\n",
            "Epoch 112/260\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.2546 - val_loss: 0.2578\n",
            "Epoch 113/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 114/260\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2545 - val_loss: 0.2578\n",
            "Epoch 115/260\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2544 - val_loss: 0.2578\n",
            "Epoch 116/260\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2543 - val_loss: 0.2578\n",
            "Epoch 117/260\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2542 - val_loss: 0.2578\n",
            "Epoch 118/260\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.2541 - val_loss: 0.2578\n",
            "Epoch 119/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2541 - val_loss: 0.2578\n",
            "Epoch 120/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2540 - val_loss: 0.2578\n",
            "Epoch 121/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2539 - val_loss: 0.2578\n",
            "Epoch 122/260\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.2538 - val_loss: 0.2578\n",
            "Epoch 123/260\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2537 - val_loss: 0.2578\n",
            "Epoch 124/260\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.2536 - val_loss: 0.2578\n",
            "Epoch 125/260\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2536 - val_loss: 0.2578\n",
            "Epoch 126/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2535 - val_loss: 0.2578\n",
            "Epoch 127/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2534 - val_loss: 0.2578\n",
            "Epoch 128/260\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2533 - val_loss: 0.2578\n",
            "Epoch 129/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2532 - val_loss: 0.2578\n",
            "Epoch 130/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2532 - val_loss: 0.2578\n",
            "Epoch 131/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2531 - val_loss: 0.2578\n",
            "Epoch 132/260\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2530 - val_loss: 0.2578\n",
            "Epoch 133/260\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.2529 - val_loss: 0.2578\n",
            "Epoch 134/260\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2528 - val_loss: 0.2578\n",
            "Epoch 135/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2527 - val_loss: 0.2578\n",
            "Epoch 136/260\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2527 - val_loss: 0.2578\n",
            "Epoch 137/260\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.2526 - val_loss: 0.2578\n",
            "Epoch 138/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2525 - val_loss: 0.2578\n",
            "Epoch 139/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2524 - val_loss: 0.2578\n",
            "Epoch 140/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2523 - val_loss: 0.2578\n",
            "Epoch 141/260\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2523 - val_loss: 0.2578\n",
            "Epoch 142/260\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2522 - val_loss: 0.2578\n",
            "Epoch 143/260\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2521 - val_loss: 0.2578\n",
            "Epoch 144/260\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.2520 - val_loss: 0.2578\n",
            "Epoch 145/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2520 - val_loss: 0.2578\n",
            "Epoch 146/260\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2519 - val_loss: 0.2578\n",
            "Epoch 147/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 148/260\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 149/260\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2518 - val_loss: 0.2578\n",
            "Epoch 150/260\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2517 - val_loss: 0.2578\n",
            "Epoch 151/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2516 - val_loss: 0.2578\n",
            "Epoch 152/260\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2516 - val_loss: 0.2578\n",
            "Epoch 153/260\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.2515 - val_loss: 0.2578\n",
            "Epoch 154/260\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2514 - val_loss: 0.2578\n",
            "Epoch 155/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2513 - val_loss: 0.2578\n",
            "Epoch 156/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2512 - val_loss: 0.2578\n",
            "Epoch 157/260\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2512 - val_loss: 0.2578\n",
            "Epoch 158/260\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.2511 - val_loss: 0.2578\n",
            "Epoch 159/260\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.2510 - val_loss: 0.2578\n",
            "Epoch 160/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2509 - val_loss: 0.2578\n",
            "Epoch 161/260\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2509 - val_loss: 0.2578\n",
            "Epoch 162/260\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.2508 - val_loss: 0.2578\n",
            "Epoch 163/260\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2507 - val_loss: 0.2578\n",
            "Epoch 164/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2506 - val_loss: 0.2578\n",
            "Epoch 165/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 166/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2505 - val_loss: 0.2578\n",
            "Epoch 167/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2505 - val_loss: 0.2578\n",
            "Epoch 168/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 169/260\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.2504 - val_loss: 0.2578\n",
            "Epoch 170/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2503 - val_loss: 0.2578\n",
            "Epoch 171/260\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.2502 - val_loss: 0.2578\n",
            "Epoch 172/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2502 - val_loss: 0.2578\n",
            "Epoch 173/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2501 - val_loss: 0.2578\n",
            "Epoch 174/260\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 175/260\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 176/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 177/260\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 178/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 179/260\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 180/260\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 181/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 182/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 183/260\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 184/260\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 185/260\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 186/260\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 187/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 188/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 189/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 190/260\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 191/260\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 192/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 193/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 194/260\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 195/260\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 196/260\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 197/260\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 198/260\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 199/260\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 200/260\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 201/260\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 202/260\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 203/260\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 204/260\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 205/260\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 206/260\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 207/260\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 208/260\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 209/260\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 210/260\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 211/260\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 212/260\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 213/260\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 214/260\n",
            "1/1 [==============================] - 0s 186ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 215/260\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 216/260\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 217/260\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 218/260\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 219/260\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 220/260\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 221/260\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 222/260\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 223/260\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 224/260\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 225/260\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 226/260\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 227/260\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 228/260\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 229/260\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 230/260\n",
            "1/1 [==============================] - 0s 148ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 231/260\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 232/260\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 233/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 234/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 235/260\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 236/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 237/260\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 238/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 239/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 240/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 241/260\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 242/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 243/260\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 244/260\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 245/260\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 246/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 247/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 248/260\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 249/260\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 250/260\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 251/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 252/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 253/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2500 - val_loss: 0.2578\n",
            "Epoch 254/260\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 255/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 256/260\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 257/260\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 258/260\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 259/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2499 - val_loss: 0.2578\n",
            "Epoch 260/260\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.2499 - val_loss: 0.2578\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f624ff572d0>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TESTING ML PROGRAM\")\n",
        "model.evaluate(\n",
        "\tx=np.array(bts1.loc[:,\"X3\":\"X0\"]),\n",
        "\ty=np.array(bts1.loc[:,\"Y1\":\"Y0\"])\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbriGYR4a_Ul",
        "outputId": "5664882d-f959-4b57-ffdb-dc7f35ef581f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TESTING ML PROGRAM\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.2499\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.24988582730293274"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PREDICTION ML PROGRAM\")\n",
        "predictions = model.predict(bps1)\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "\tprint(str(bps1[i]) + \" => \" + str(predictions[i]) + \" => \" + str(np.round(predictions[i])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76rgVADObIT0",
        "outputId": "e45d5dce-fc06-4679-bcf4-111bab8ef92d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f624fdd9cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREDICTION ML PROGRAM\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "[0 1 1 1] => [0.49979615 0.501375  ] => [0. 1.]\n",
            "[1 0 1 1] => [0.5087891  0.51564765] => [1. 1.]\n"
          ]
        }
      ]
    }
  ]
}